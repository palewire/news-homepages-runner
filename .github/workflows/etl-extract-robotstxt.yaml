name: "Scheduled ETL: Extract robots.txt sample"

on:
  schedule:
    - cron: '30 0 * * *'
  workflow_dispatch:

concurrency:
  group: extract-robotstxt
  cancel-in-progress: true

jobs:
  download:
    name: Download robots.txt files
    runs-on: ubuntu-latest
    steps:
       - id: checkout
         name: Checkout
         uses: actions/checkout@v4

       - id: install
         name: Install
         uses: ./.github/actions/install

       - id: cache
         name: Update cache
         uses: actions/cache@v3
         with:
           path: ~/.cache
           key: extract-robotstxt-${{ github.run_id }}
           restore-keys: |
             extract-robotstxt

       - id: download-robotstxt
         name: Download robots.txt files
         run: pipenv run newshomepages-extract robotstxt --days=7 --output-path=./robotstxt-sample.csv;
         shell: bash

       - id: save
         name: Save artifact
         uses: actions/upload-artifact@v3
         with:
           name: extracts
           path: ./robotstxt-sample.csv
           if-no-files-found: error

       - id: upload-to-internet-archive
         name: Upload files to archive.org
         uses: palewire/internet-archive-upload@v1
         with:
           access-key: ${{ secrets.IA_ACCESS_KEY }}
           secret-key: ${{ secrets.IA_SECRET_KEY }}
           identifier: news-homepages-extracts
           files: robotstxt-sample.csv
