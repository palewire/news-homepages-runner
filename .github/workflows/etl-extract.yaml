name: "Scheduled ETL: Extracts"

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

concurrency:
  group: extract
  cancel-in-progress: true

jobs:
  download-lighthouse:
    name: Download Lighthouse
    runs-on: ubuntu-latest
    steps:
      - id: checkout
        name: Checkout
        uses: actions/checkout@v3

      - id: install
        name: Install
        uses: ./.github/actions/install

      - id: cache
        name: Update cache
        uses: actions/cache@v3
        with:
          path: .cache
          key: extract-lighthouse-${{ github.run_id }}
          restore-keys: |
            extract-lighthouse

      - id: download-lighthouse
        name: Download Lighthouse files
        run: pipenv run python -m newshomepages.extract download-lighthouse --days=7 --output-path=./lighthouse-sample.csv;
        shell: bash

      - id: save
        name: Save artifact
        uses: actions/upload-artifact@v3
        with:
          name: extracts
          path: ./lighthouse-sample.csv
          if-no-files-found: error

  # download-drudge-hyperlinks:
  #   name: Download Drudge hyperlinks
  #   runs-on: ubuntu-latest
  #   steps:
  #     - id: checkout
  #       name: Checkout
  #       uses: actions/checkout@v3

  #     - id: install
  #       name: Install
  #       uses: ./.github/actions/install

  #     - id: cache
  #       name: Update cache
  #       uses: actions/cache@v3
  #       with:
  #         path: .cache
  #         key: extract-drudge-${{ github.run_id }}
  #         restore-keys: |
  #           extract-drudge

  #     - id: download-drudge-items
  #       name: Download Drudge items
  #       run: pipenv run newshomepages-extract download-items --site=drudge
  #       shell: bash
  #       env:
  #         IA_ACCESS_KEY: ${{ secrets.IA_ACCESS_KEY }}
  #         IA_SECRET_KEY: ${{ secrets.IA_SECRET_KEY }}
  #         IA_COLLECTION: ${{ secrets.IA_COLLECTION }}

  #     - id: consolidate-items
  #       name: Consolidate items
  #       run: pipenv run newshomepages-extract consolidate
  #       shell: bash

  #     - id: download-drudge-hyperlinks
  #       name: Download Drudge hyperlink lists
  #       run: pipenv run newshomepages-extract download-hyperlinks --site=drudge --days=90 --output-path=./drudge-hyperlinks-sample.csv;
  #       shell: bash

  #     - id: save
  #       name: Save artifact
  #       uses: actions/upload-artifact@v3
  #       with:
  #         name: extracts
  #         path: ./drudge-hyperlinks-sample.csv
  #         if-no-files-found: error

  publish:
    name: Publish
    runs-on: ubuntu-latest
    timeout-minutes: 60
    continue-on-error: true
    needs: [download-lighthouse,] #, download-drudge-hyperlinks]
    steps:
      - id: checkout
        name: Checkout
        uses: actions/checkout@v3

      - id: upload
        name: Upload
        uses: ./.github/actions/upload
        with:
          artifact: extracts
          destination: extracts/csv/
          s3-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          s3-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          s3-bucket-name: ${{ secrets.AWS_BUCKET_NAME }}
